{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce the data-closed-loop tools, \n",
    "\n",
    "including data engine and data-driven planning tools.\n",
    "\n",
    "Before reading this, it is recommended to get through the interface.ipynb.\n",
    "\n",
    "It is based on spider-python==0.1.16.4\n",
    "\n",
    "If you are using a different version, the code may not work as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How to collect data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython module not found. Use pyximport temporally\n",
      "Using matplotlib backend: <object object at 0x000001EFFE9FACE0>\n",
      "LogBuffer: Log Buffer is listening to the planner.\n",
      "LogBuffer: All data will be automatically recorded...\n",
      "A segment containing 100 log records has been saved to ./dataset/log_segment0000\n",
      "A segment containing 100 log records has been saved to ./dataset/log_segment0001\n",
      "A segment containing 100 log records has been saved to ./dataset/log_segment0002\n",
      "A segment containing 100 log records has been saved to ./dataset/log_segment0003\n"
     ]
    }
   ],
   "source": [
    "import spider\n",
    "from spider.interface.BaseBenchmark import DummyBenchmark\n",
    "from spider.planner_zoo import LatticePlanner\n",
    "from spider.data.DataBuffer import LogBuffer\n",
    "\n",
    "%matplotlib\n",
    "\n",
    "# 用log_buffer.apply_to(planner)启用\n",
    "benchmark = DummyBenchmark({\n",
    "    \"snapshot\": False,\n",
    "    \"map_frequency\": 1, # 记录地图数据\n",
    "    # \"racetrack\": \"straight\",\n",
    "})\n",
    "\n",
    "planner = LatticePlanner({\n",
    "    \"steps\": 20,\n",
    "    \"dt\": 0.2,\n",
    "    \"print_info\": False\n",
    "})\n",
    "\n",
    "log_buffer = LogBuffer(\n",
    "    autosave_max_intervals=100,\n",
    "    file_format=spider.DATA_FORMAT_RAW,\n",
    "    # file_format=spider.DATA_FORMAT_JSON,\n",
    "    data_root='./dataset/'\n",
    ")\n",
    "\n",
    "log_buffer.apply_to(planner)\n",
    "\n",
    "for episode in range(5):\n",
    "    benchmark.test(planner)\n",
    "\n",
    "log_buffer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to train IL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spider.data.Dataset import OfflineLogDataset\n",
    "from spider.planner_zoo.MlpPlanner import MlpPlanner\n",
    "\n",
    "\n",
    "# test the planner\n",
    "train = 0\n",
    "test_mode_closed_loop = 1\n",
    "\n",
    "# setup the planner\n",
    "planner = MlpPlanner({\n",
    "    \"steps\": 20,\n",
    "    \"dt\": 0.2,\n",
    "    \"num_object\": 5,\n",
    "    \"normalize\": False,\n",
    "    \"relative\": False,\n",
    "    \"longitudinal_range\": (-50, 100),\n",
    "    \"lateral_range\": (-20,20),\n",
    "\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"enable_tensorboard\": True,\n",
    "    \"tensorboard_root\": './tensorboard/'\n",
    "})\n",
    "\n",
    "# setup the dataset\n",
    "dataset = OfflineLogDataset('./dataset/', planner.state_encoder, planner.action_encoder)\n",
    "train_loader = dataset.get_dataloader(batch_size=64, shuffle=True)  #DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_mode the planner\n",
    "if train:\n",
    "    planner.policy.learn_dataset(100, train_loader=train_loader)\n",
    "\n",
    "    # save the model\n",
    "    planner.save_state_dict('mlp.pth')\n",
    "\n",
    "# load the model\n",
    "planner.load_state_dict('mlp.pth')\n",
    "\n",
    "\n",
    "if test_mode_closed_loop:\n",
    "    from spider.interface.BaseBenchmark import DummyBenchmark\n",
    "    benchmark = DummyBenchmark({\n",
    "        \"save_video\": True,\n",
    "    })\n",
    "    benchmark.test(planner)\n",
    "else:\n",
    "    dataset.replay(planner, 0, recording=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How to train RL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spider.visualize as vis\n",
    "import tqdm\n",
    "from spider.interface import DummyInterface, DummyBenchmark\n",
    "from spider.planner_zoo.DQNPlanner import DQNPlanner\n",
    "from spider.rl.reward.TrajectoryReward import TrajectoryReward\n",
    "\n",
    "class Trainner:\n",
    "    '''\n",
    "    todo:以后加一个把环境打包成gym环境的功能\n",
    "    '''\n",
    "    def __init__(self, env_interface, reward_function, visualize=False):\n",
    "        self.env_interface = env_interface\n",
    "        self.reward_function = reward_function\n",
    "        self._visualize = visualize\n",
    "\n",
    "\n",
    "    def train(self, planner, train_steps, batch_size=64):\n",
    "        # todo: 是一个step触发训练，还是一个episode触发训练？\n",
    "        #  以及一轮训练的次数是1吗？可以参考stable baselines3\n",
    "\n",
    "        policy = planner.policy\n",
    "        exp_buffer = planner.exp_buffer\n",
    "\n",
    "        exp_buffer.apply_to(policy, self.reward_function)  # 开始监听\n",
    "\n",
    "        obs, done = None, True\n",
    "\n",
    "        policy.set_exploration(enable=True)\n",
    "\n",
    "        for i in tqdm.tqdm(range(train_steps)):\n",
    "            if done:\n",
    "                obs = self.env_interface.reset()\n",
    "\n",
    "            # forward\n",
    "            plan = planner.plan(*obs) # 监听exp_buffer记录了obs, plan\n",
    "            self.env_interface.conduct_trajectory(plan)\n",
    "            obs2 = self.env_interface.wrap_observation()\n",
    "\n",
    "            # feedback\n",
    "            reward, done = self.reward_function.evaluate_log(obs, plan, obs2) # 监听exp_buffer记录了reward, done\n",
    "            policy.try_write_reward(reward, done, i)\n",
    "\n",
    "            # 学习\n",
    "            batched_data = exp_buffer.sample(batch_size)\n",
    "            policy.learn_batch(*batched_data)\n",
    "\n",
    "            # visualize\n",
    "            if self._visualize:\n",
    "                vis.cla()\n",
    "                vis.lazy_draw(*obs, plan)\n",
    "                vis.title(f\"Step {i}, Reward {reward}\")\n",
    "                vis.pause(0.001)\n",
    "\n",
    "            obs = obs2\n",
    "\n",
    "        policy.set_exploration(enable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# presets\n",
    "ego_size = (5.,2.)\n",
    "\n",
    "# setup env\n",
    "env_interface = DummyInterface()\n",
    "\n",
    "# setup reward\n",
    "reward_function = TrajectoryReward(\n",
    "    (-10., 280.), (-15, 15), (240., 280.), (-10,10), ego_size\n",
    ")\n",
    "\n",
    "# setup_planner\n",
    "planner_dqn = DQNPlanner({\n",
    "    \"ego_veh_width\": ego_size[1],\n",
    "    \"ego_veh_length\": ego_size[0],\n",
    "    \"enable_tensorboard\": True,\n",
    "})\n",
    "\n",
    "planner_school = Trainner(env_interface, reward_function, visualize=False)\n",
    "planner_school.train(planner_dqn, 50000, 64)\n",
    "planner_dqn.policy.save_model('./q_net.pth')\n",
    "\n",
    "planner_dqn.policy.load_model('./q_net.pth')\n",
    "DummyBenchmark({\"save_video\": True,}).test(planner_dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
